{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hl/anaconda2/lib/python2.7/site-packages/matplotlib/font_manager.py:279: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  'Matplotlib is building the font cache using fc-list. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noisy topics\n",
      "[('we love berXers', 7), ('we hate sandwicXes', 5), ('we love beXgers', 3), ('we hate sanXwiches', 2), ('we hate sXndwiches', 2), ('we hate sandwichXs', 2), ('we hate sandwiXhes', 2), ('we love bergeXs', 2), ('we love Xergers', 1), ('we love bergXrs', 1), ('we hate sandwXches', 1), ('we love bXrgers', 1), ('we hate saXdwiches', 1)]\n",
      "\n",
      "clearcut topics\n",
      "[('we love bergers', 1000), ('we hate sandwiches', 1000)]\n",
      "\n",
      "unbalanced topics\n",
      "[('we hate sandwiches', 1000), ('we love bergers', 10)]\n",
      "\n",
      "semantic topics\n",
      "[('we love bergers', 1000), ('we love sandwiches', 1000), ('we hate sandwiches', 1000), ('we hate bergers', 1000)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#LDA NMF SVD KMeans\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, NMF, LatentDirichletAllocation\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import Counter\n",
    "\n",
    "def generate_clearcut_topics():\n",
    "    ## for demostration purpose, don't take it personally : )\n",
    "    return np.repeat([\"we love bergers\", \"we hate sandwiches\"], [1000, 1000])\n",
    "\n",
    "def generate_unbalanced_topics():\n",
    "    return np.repeat([\"we love bergers\", \"we hate sandwiches\"], [10, 1000])\n",
    "\n",
    "def generate_semantic_context_topics():\n",
    "    return np.repeat([\"we love bergers\"\n",
    "                      , \"we hate bergers\"\n",
    "                      , \"we love sandwiches\"\n",
    "                      , \"we hate sandwiches\"], 1000)\n",
    "\n",
    "def generate_noisy_topics():\n",
    "    def _random_typos(word, n):\n",
    "        typo_index = np.random.randint(0, len(word), n)\n",
    "        return [word[:i]+\"X\"+word[i+1:] for i in typo_index]\n",
    "    t1 = [\"we love %s\" % w for w in _random_typos(\"bergers\", 15)]\n",
    "    t2 = [\"we hate %s\" % w for w in _random_typos(\"sandwiches\", 15)]\n",
    "    return np.r_[t1, t2] #在单词的随机位置添加X\n",
    "\n",
    "sample_texts = {\n",
    "     \"clearcut topics\": generate_clearcut_topics()\n",
    "    , \"unbalanced topics\": generate_unbalanced_topics()\n",
    "    , \"semantic topics\": generate_semantic_context_topics()\n",
    "    , \"noisy topics\": generate_noisy_topics()\n",
    "}\n",
    "\n",
    "\n",
    "for desc, texts in sample_texts.items():\n",
    "    print desc\n",
    "    print Counter(texts).most_common()\n",
    "    print \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
